diff --git a/TODO b/TODO
index c2e19af..51fa559 100644
--- a/TODO
+++ b/TODO
@@ -293,3 +293,142 @@ VIA Rail MITM's SSL In Ottawa:
 Jul 22 17:47:21.983 [Warning] Problem bootstrapping. Stuck at 85%: Finishing handshake with first hop. (DONE; DONE; count 13; recommendation warn)
 
 http://wireless.colubris.com:81/goform/HtmlLoginRequest?username=al1852&password=al1852
+
+VIA Rail Via header:
+
+HTTP/1.0 301 Moved Permanently
+Location: http://www.google.com/
+Content-Type: text/html; charset=UTF-8
+Date: Sat, 23 Jul 2011 02:21:30 GMT
+Expires: Mon, 22 Aug 2011 02:21:30 GMT
+Cache-Control: public, max-age=2592000
+Server: gws
+Content-Length: 219
+X-XSS-Protection: 1; mode=block
+X-Cache: MISS from cache_server
+X-Cache-Lookup: MISS from cache_server:3128
+Via: 1.0 cache_server:3128 (squid/2.6.STABLE21)
+Connection: close
+
+<HTML><HEAD><meta http-equiv="content-type" content="text/html;charset=utf-8">
+<TITLE>301 Moved</TITLE></HEAD><BODY>
+<H1>301 Moved</H1>
+The document has moved
+<A HREF="http://www.google.com/">here</A>.
+</BODY></HTML>
+
+
+blocked site:
+
+HTTP/1.0 302 Moved Temporarily
+Server: squid/2.6.STABLE21
+Date: Sat, 23 Jul 2011 02:22:17 GMT
+Content-Length: 0
+Location: http://10.66.66.66/denied.html
+
+invalid request response:
+
+$ nc 8.8.8.8 80
+hjdashjkdsahjkdsa
+HTTP/1.0 400 Bad Request
+Server: squid/2.6.STABLE21
+Date: Sat, 23 Jul 2011 02:22:44 GMT
+Content-Type: text/html
+Content-Length: 1178
+Expires: Sat, 23 Jul 2011 02:22:44 GMT
+X-Squid-Error: ERR_INVALID_REQ 0
+X-Cache: MISS from cache_server
+X-Cache-Lookup: NONE from cache_server:3128
+Via: 1.0 cache_server:3128 (squid/2.6.STABLE21)
+Proxy-Connection: close
+
+<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
+<HTML><HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
+<TITLE>ERROR: The requested URL could not be retrieved</TITLE>
+<STYLE type="text/css"><!--BODY{background-color:#ffffff;font-family:verdana,sans-serif}PRE{font-family:sans-serif}--></STYLE>
+</HEAD><BODY>
+<H1>ERROR</H1>
+<H2>The requested URL could not be retrieved</H2>
+<HR noshade size="1px">
+<P>
+While trying to process the request:
+<PRE>
+hjdashjkdsahjkdsa
+
+</PRE>
+<P>
+The following error was encountered:
+<UL>
+<LI>
+<STRONG>
+Invalid Request
+</STRONG>
+</UL>
+
+<P>
+Some aspect of the HTTP Request is invalid.  Possible problems:
+<UL>
+<LI>Missing or unknown request method
+<LI>Missing URL
+<LI>Missing HTTP Identifier (HTTP/1.0)
+<LI>Request is too large
+<LI>Content-Length missing for POST or PUT requests
+<LI>Illegal character in hostname; underscores are not allowed
+</UL>
+<P>Your cache administrator is <A HREF="mailto:root">root</A>. 
+
+<BR clear="all">
+<HR noshade size="1px">
+<ADDRESS>
+Generated Sat, 23 Jul 2011 02:22:44 GMT by cache_server (squid/2.6.STABLE21)
+</ADDRESS>
+</BODY></HTML>
+
+nc 10.66.66.66 80
+GET cache_object://localhost/info HTTP/1.0
+HTTP/1.0 403 Forbidden
+Server: squid/2.6.STABLE21
+Date: Sat, 23 Jul 2011 02:25:56 GMT
+Content-Type: text/html
+Content-Length: 1061
+Expires: Sat, 23 Jul 2011 02:25:56 GMT
+X-Squid-Error: ERR_ACCESS_DENIED 0
+X-Cache: MISS from cache_server
+X-Cache-Lookup: NONE from cache_server:3128
+Via: 1.0 cache_server:3128 (squid/2.6.STABLE21)
+Proxy-Connection: close
+
+<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
+<HTML><HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
+<TITLE>ERROR: The requested URL could not be retrieved</TITLE>
+<STYLE type="text/css"><!--BODY{background-color:#ffffff;font-family:verdana,sans-serif}PRE{font-family:sans-serif}--></STYLE>
+</HEAD><BODY>
+<H1>ERROR</H1>
+<H2>The requested URL could not be retrieved</H2>
+<HR noshade size="1px">
+<P>
+While trying to retrieve the URL:
+<A HREF="cache_object://localhost/info">cache_object://localhost/info</A>
+<P>
+The following error was encountered:
+<UL>
+<LI>
+<STRONG>
+Access Denied.
+</STRONG>
+<P>
+Access control configuration prevents your request from
+being allowed at this time.  Please contact your service provider if
+you feel this is incorrect.
+</UL>
+<P>Your cache administrator is <A HREF="mailto:root">root</A>. 
+
+
+<BR clear="all">
+<HR noshade size="1px">
+<ADDRESS>
+Generated Sat, 23 Jul 2011 02:25:56 GMT by cache_server (squid/2.6.STABLE21)
+</ADDRESS>
+</BODY></HTML>
+
+
diff --git a/ooni/command.py b/ooni/command.py
index 361190f..df1a58c 100644
--- a/ooni/command.py
+++ b/ooni/command.py
@@ -13,6 +13,7 @@ import ooni.captive_portal
 import ooni.namecheck
 import ooni.dns_poisoning
 import ooni.dns_cc_check
+import ooni.transparenthttp
 
 class Command():
     def __init__(self, args):
@@ -48,6 +49,15 @@ class Command():
             help="run captiveportal tests"
         )
 
+        # --transhttp
+        def cb_transhttp(option, opt, value, oparser):
+            self.action = opt[2:]
+        optparser.add_option(
+            "--transhttp",
+            action="callback", callback=cb_transhttp,
+            help="run Transparent HTTP tests"
+        )
+
         # --dns
         def cb_dnstests(option, opt, value, oparser):
             self.action = opt[2:]
@@ -122,7 +132,7 @@ class Command():
             if (not self.action):
                 raise optparse.OptionError(
                     'is required',
-                    '--dns | --dnsbulk | --captiveportal | --help | --version'
+                    '--dns | --dnsbulk | --dnscccheck | [ --cc CC ] | --captiveportal | --transhttp | --help | --version'
                 )
 
         except optparse.OptionError, err:
@@ -138,6 +148,10 @@ class Command():
         captive_portal = ooni.captive_portal.CaptivePortal
         captive_portal(self).main()
 
+    def transhttp(self):
+        transparent_http = ooni.transparenthttp.TransparentHTTPProxy
+        transparent_http(self).main()
+
     def dns(self):
         dnstests = ooni.namecheck.DNS
         dnstests(self).main()
diff --git a/ooni/dns.py b/ooni/dns.py
index 95da6ef..90d50bd 100644
--- a/ooni/dns.py
+++ b/ooni/dns.py
@@ -8,7 +8,7 @@ from socket import gethostbyname
 import ooni.common
 
 # apt-get install python-dns
-import DNS
+import dns
 import random
 
 """ Wrap gethostbyname """
diff --git a/ooni/http.py b/ooni/http.py
index 62365bb..bb72001 100644
--- a/ooni/http.py
+++ b/ooni/http.py
@@ -7,8 +7,14 @@
 from socket import gethostbyname
 import ooni.common
 import urllib2
+import httplib
+from urlparse import urlparse
+from pprint import pprint
 import pycurl
+import random
+import string
 import re
+from BeautifulSoup import BeautifulSoup
 
 # By default, we'll be Torbutton's UA
 default_ua = { 'User-Agent' : 
@@ -20,20 +26,8 @@ default_proxy_type = PROXYTYPE_SOCKS5
 default_proxy_host = "127.0.0.1"
 default_proxy_port = "9050"
 
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+#class HTTPResponse(object):
+#  def __init__(self):
 
 
 """A very basic HTTP fetcher that uses Tor by default and returns a curl
@@ -51,7 +45,7 @@ def http_proxy_fetch(url, headers, proxy_type=5,
    http_code = getinfo(pycurl.HTTP_CODE)
    return response, http_code
 
-"""A very basic HTTP fetcher that returns a urllib3 response object."""
+"""A very basic HTTP fetcher that returns a urllib2 response object."""
 def http_fetch(url, 
                headers= default_ua,
                label="generic HTTP fetch"):
@@ -136,6 +130,76 @@ def http_header_no_match(experiment_url, control_header, control_result):
   else:
     return True
 
+def http_request(self, method, url, path=None):
+  """Takes as argument url that is perfectly formed (http://hostname/REQUEST"""
+  purl = urlparse(url)
+  host = purl.netloc
+  conn = httplib.HTTPConnection(host, 80)
+  if path is None:
+    path = purl.path
+  conn.request(method, purl.path)
+  response = conn.getresponse()
+  headers = dict(response.getheaders())
+  self.headers = headers
+  self.data = response.read()
+  return True
+
+def search_headers(self, s_headers, url):
+  if http_request(self, "GET", url):
+    headers = self.headers
+  else:
+    return None
+  result = {}
+  for h in s_headers.items():
+    result[h[0]] = h[0] in headers
+  return result
+
+def http_header_match_dict(experimental_url, dict_header):
+  result = {}
+  url_header = http_get_header_dict(experimental_url)
+
+# XXX for testing
+#  [('content-length', '9291'), ('via', '1.0 cache_server:3128 (squid/2.6.STABLE21)'), ('x-cache', 'MISS from cache_server'), ('accept-ranges', 'bytes'), ('server', 'Apache/2.2.16 (Debian)'), ('last-modified', 'Fri, 22 Jul 2011 03:00:31 GMT'), ('connection', 'close'), ('etag', '"105801a-244b-4a89fab1e51c0;49e684ba90c80"'), ('date', 'Sat, 23 Jul 2011 03:03:56 GMT'), ('content-type', 'text/html'), ('x-cache-lookup', 'MISS from cache_server:3128')]   
+      
+def search_squid_headers(self):
+  url = "http://securityfocus.org/blabla"
+  s_headers = {'via': '1.0 cache_server:3128 (squid/2.6.STABLE21)', 'x-cache': 'MISS from cache_server', 'x-cache-lookup':'MISS from cache_server:3128'}
+  ret = search_headers(self, s_headers, url)
+  for i in ret.items():
+    if i[1] is True:
+      return False
+  return True
+
+def random_bad_request(self):
+  url = "http://securityfocus.org/blabla"
+  r_str = ''.join(random.choice(string.ascii_uppercase + string.digits) for x in range(random.randint(5,20)))
+  if http_request(self, r_str, url):
+    return True
+  else:
+    return None
+
+def squid_search_bad_request(self):
+  if random_bad_request(self):
+    s_headers = {'X-Squid-Error' : 'ERR_INVALID_REQ 0'}
+    for i in s_headers.items():
+      if i[0] in self.headers:
+        return False
+    return True
+  else:
+    return None
+
+def squid_cacheobject_request(self):
+  url = "http://securityfocus.org/blabla"
+  if http_request(self, "GET", url, "cache_object://localhost/info"):
+    soup = BeautifulSoup(self.data)
+    if soup.find('strong') and soup.find('strong').string == "Access Denied.":
+      return False
+    else:
+      return True
+  else:
+    return None
+  
+
 def MSHTTP_CP_Tests(self):
   experiment_url = "http://www.msftncsi.com/ncsi.txt"
   expectedResponse = "Microsoft NCSI" # Only this - nothing more
@@ -186,6 +250,18 @@ def WC3_CP_Tests(self):
 
 # Google ChromeOS fetches this url in guest mode
 # and they expect the user to authenticate
-  def googleChromeOSHTTPTest(self):
-    print "noop"
-    #url = "http://www.google.com/"
+def googleChromeOSHTTPTest(self):
+  print "noop"
+  #url = "http://www.google.com/"
+
+def SquidHeader_TransparentHTTP_Tests(self):
+  return search_squid_headers(self)
+
+def SquidBadRequest_TransparentHTTP_Tests(self):
+  squid_cacheobject_request(self)
+  return squid_search_bad_request(self)    
+
+def SquidCacheobject_TransparentHTTP_Tests(self):
+  return squid_cacheobject_request(self)
+
+
